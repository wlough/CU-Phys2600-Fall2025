{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-100899e0deea9789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Tutorial 23: gvar and lsqfit\n",
    "\n",
    "## PHYS 2600, Fall 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import cell\n",
    "%pip install -q gvar lsqfit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gvar as gv\n",
    "import lsqfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0024af98da1a06ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## T23.1 - Basics of gvar\n",
    "\n",
    "### Part A\n",
    "\n",
    "Let's start with just making `gvar`s and doing simple arithmetic on them.  Enter the following values as `gvar` objects:\n",
    "\n",
    "* x = 2.19(52)\n",
    "* y = 0.93(13)\n",
    "\n",
    "Try using both the string and numeric mean/error versions of `gv.gvar()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f312487a912d9acb",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab7dca2b566988ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, calculate the following quantities and print them out as `gvar`s:\n",
    "\n",
    "* $w = x+y$\n",
    "* $z = xy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ca26ae84a201b18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc8831497c1701a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part B\n",
    "\n",
    "Next, verify that the propagation of error matches what you would expect from the analytic formulas:\n",
    "\n",
    "$$\n",
    "\\sigma_{w} = \\sqrt{\\sigma_x^2 + \\sigma_y^2} \\\\\n",
    "\\frac{\\sigma_{z}}{z} = \\sqrt{\\frac{\\sigma_x^2}{x^2} + \\frac{\\sigma_y^2}{y^2}}\n",
    "$$\n",
    "\n",
    "In order to calculate these formulas, you'll need to access the mean values and error bars individually from `x` and `y`.  To do this, you can use the `.mean` or `.sdev` properties, or the `gv.mean()` or `gv.sdev()` functions.\n",
    "\n",
    "(In other words: if `g` is a `gvar` object, either `g.mean` or `gv.mean(g)` will give you its mean value as a number.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7bdf23329c6afa64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Checking error propagation\n",
    "print(w.sdev)\n",
    "print(np.sqrt(x.sdev**2 + y.sdev**2))\n",
    "\n",
    "print(z.sdev)\n",
    "print(z.mean * np.sqrt( (x.sdev / x.mean)**2 + (y.sdev / y.mean)**2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2325a011380f6544",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use `gv.corr()` to find the _correlation coefficient_ between `w` and `z` - it should be non-zero since they were both produced from the same Gaussian variables, `x` and `y`!\n",
    "\n",
    "Calculate the ratio `w/z`.  Then create two _brand new_ `gvar` variables, `w_new` and `z_new`, which have the same mean and error values as `w` and `z`.  Show that `w_new/z_new` has a much larger error bar than `w/z`, due to the missing correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dcdabb8157f9f846",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-041a45e5ac1f3cb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part C\n",
    "\n",
    "Sometimes, it's more convenient to build many `gvar` objects at once from separate lists of means and errors.  To do this, we just pass matching lists to the `gv.gvar()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32975ea7e0f8fbae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_means = [3.2,5.4,3.7,1.2,1.7]\n",
    "some_errors = [1.9,0.4,1.1,0.5,0.9]\n",
    "\n",
    "some_gv = gv.gvar(some_means, some_errors)\n",
    "print(some_gv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9636641acd680ff0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice that this automatically gives us an array of `gvars`.  One nice thing about `gvar` is that it plays very nicely with `numpy`, so we can do all the usual array operations when we have lists like this.\n",
    "\n",
    "First, a warm-up: use NumPy to __compute the mean of `some_gv`__.  (You should find `3.04(49)`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-046aaa4b121e62f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8b11897c7049a27e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also use masks with arrays of `gvar` objects.  In the cell below, __create a mask__ to select all of the elements of `some_gv` which satisfy $\\mu / \\sigma > 2$ - i.e., anything whose central value is at least twice as large as its error bar.  (Another way to state this is that we are selecting point with \"_signal-to-noise ratio_\" SNR of at least 2.)\n",
    "\n",
    "_(Hint: `gv.mean()` and `gv.sdev()` are happy to act on entire arrays of `gvar` objects at once, returning a matching array of mean values or error values.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b691b23b0a4482f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af728a2201567e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should see the following print-out:\n",
    "\n",
    "```\n",
    "[5.40(40) 3.7(1.1) 1.20(50)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7962c4e129aca38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## T23.2 - Getting started with `lsqfit`\n",
    "\n",
    "As usual, there are a lot of available modules that can help us carry out least-squares modeling in Python - for example, the `scipy.optimize.curve_fit` function can do general non-linear least squares.\n",
    "\n",
    "We're going to learn a different module called `lsqfit` - you can find [the documentation here](https://lsqfit.readthedocs.io/en/latest/).  The `lsqfit` module is something I use in research often: it is more general and powerful than the `scipy` curve fitter, and it works natively with the `gvar` module (both are from the same author.)\n",
    "\n",
    "To warm up, we'll start with a simple linear fit to some real data taken from a teaching photoelectric effect experiment.  Here are the data points:\n",
    "\n",
    "| f [$10^{14}$ Hz] | $V_{\\rm stop}$ [V] |\n",
    "|-------------------------|--------------------|\n",
    "| 5.490 | 0.24(4) |\n",
    "| 6.879 | 0.80(10) |\n",
    "| 7.408 | 1.20(20) |\n",
    "| 8.213 | 1.50(25) |\n",
    "\n",
    "I've also provided the data in the cell below, along with a quick function that makes an errorbar plot of gvar data.  __Run the cell to load the data variables and make a plot.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-92bdb8a08bb8bb51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "f_data = np.array([5.490, 6.879, 7.408, 8.213]) # 10^{14} Hz\n",
    "Vcut = gv.gvar(['0.24(4)', '0.80(10)', '1.20(20)', '1.50(25)']) # V\n",
    "\n",
    "def plot_gvar(x, y_gv):\n",
    "    \"\"\"\n",
    "    Given two arrays of equal length, one of which contains\n",
    "    gvar objects, returns an errorbar plot of y(x).\n",
    "    \n",
    "    Arguments:\n",
    "    ====\n",
    "    x: Array of x-values.\n",
    "    y_gv: Array of y-values with error [as gvars.]\n",
    "    \n",
    "    Returns:\n",
    "    =====\n",
    "    An errorbar plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    return plt.errorbar(x, gv.mean(y_gv), yerr=gv.sdev(y_gv), linestyle='', marker='o')\n",
    "    \n",
    "    \n",
    "plot_gvar(f_data, Vcut)\n",
    "plt.xlim(4.5, 9.0)\n",
    "plt.ylim(0,2)\n",
    "plt.xlabel('f [$\\\\times 10^{14}$ Hz]')\n",
    "plt.ylabel('$V_{\\\\rm stop} [V]$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e8bb8bec8de009d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We expect a linear relationship between the light frequency $f$ and the stopping voltage $V_{\\rm stop}$ - looks plausible, although the data are a bit noisy.  To test the linear model and get the work function and Planck's constant, we need to set up a fit!\n",
    "\n",
    "### Part A\n",
    "\n",
    "The first thing we'll need is a model function.  It turns out that `lsqfit` wants this to be implemented as a Python function, although one nice feature of `lsqfit` is that we can use a dictionary to keep our parameter names straight.  __Implement the function `PE_model(f, a)` below__, where `f` should be an array of frequency values and `a` is a dictionary of fit parameters.  \n",
    "\n",
    "You should assume that `a` has two keys called `Phi` (capital-phi, written $\\Phi$) and `h`, which are the two variables in the model function we want:\n",
    "\n",
    "$$\n",
    "eV_{\\rm stop} = hf - \\Phi.\n",
    "$$\n",
    "\n",
    "The function should return the value of the right-hand side, $hf - \\Phi$.  (We'll be fitting this against the $eV_{\\rm stop}$ data; what you're returning is the _prediction_ of the model for $eV_{\\rm stop}$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bfb37d02cf9e5420",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def PE_model(f, a):    \n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c8ce39f14913a4ba",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy.testing as npt\n",
    "\n",
    "npt.assert_allclose(PE_model(5, {'h': 1, 'Phi': 1}), 4)\n",
    "npt.assert_allclose(PE_model(f_data, {'h': 0, 'Phi': 0}), [0, 0, 0, 0])\n",
    "npt.assert_allclose(PE_model(f_data, {'h': 1, 'Phi': 1}), [4.49, 5.879, 6.408, 7.213])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1fb8840a9c4dea70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now use your function `PE_model` to __make a plot showing the data__ (copy from the cell above) __as well as the model__, using a guess for the fit parameters of `(h, Phi) = (0.5, 3)` (which I came up with by staring at the plot - maybe you can do better?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7b77b6cd4d92162",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "a0 = {\n",
    "    'h': 0.5,\n",
    "    'Phi': 3,\n",
    "}\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0db4a56c6dd3297c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part B\n",
    "\n",
    "Now let's actually run the fitter and find the best fit!  The main workhorse function of `lsqfit` is called `lsqfit.nonlinear_fit()`.  (Yes, even though this is a linear fit, we can use this function - linear fits are a special case so it will still work.)  The documentation for `lsqfit.nonlinear_fit()` is pretty complicated, because it has a lot of features.  But recalling the definition of $\\chi^2$,\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_i \\left(\\frac{y_i - f(x_i, \\mathbf{a})}{\\sigma_i} \\right)^2,\n",
    "$$\n",
    "\n",
    "we expect that `lsqfit.nonlinear_fit` must take the following arguments at least:\n",
    "\n",
    "* Data for independent variable(s), $x_i$ - this should be __a NumPy array__.\n",
    "* Data for dependent variable(s), $y_i$, and error bars $\\sigma_i$ - this should be a __`gvar` array__.\n",
    "* The model function $f$ - we wrote this in part A.\n",
    "\n",
    "It will also need an __initial guess__, $\\mathbf{a}_0$, because the methods it uses to minimize the $\\chi^2$ function require a starting point.  (Having a good initial guess can often greatly improve the speed and accuracy of a nonlinear fit!)  \n",
    "\n",
    "Once we have our data `x` and `y`, model function `f`, and initial guess `a0`, the call signature is:\n",
    "\n",
    "```\n",
    "fit_result = lsqfit.nonlinear_fit(data=(x,y), fcn=f, p0=a0)\n",
    "```\n",
    "\n",
    "This will run the fit and return an object containing lots of information.  \n",
    "\n",
    "__Call the `nonlinear_fit` function below__, using the variables you defined above and the initial guess `a0` that you plotted just above.  Then __print the object returned by `nonlinear_fit`__ (I like to call it \"`fit_result`\") to see a report on the best fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a87bac69a0e8438",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# fit_result = \n",
    "print(fit_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-203d034945c25ed2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There's lots of information to unpack there.  The most important features in the report are:\n",
    "\n",
    "* At the top, the reduced $\\chi^2 / N_{\\rm dof}$, following by $N_{\\rm dof}$ in square brackets.  Finally, the quantity $Q$ is the p-value of the fit: the probability that we would draw the given experimental data if the best-fit model is true.  (This should be a good fit: reduced $\\chi^2$ should be smaller than 1, and $Q$ should be pretty large.)\n",
    "* Under the heading \"Parameters:\" on the left, the best-fit value with error bar for each parameter.  On the right, the initial values of each parameter.  (The \"error bar\" on our initial guess is infinity, because our guess had no error.  If we had __prior information__ about one or more parameters, we could impose that as a prior constraint on the fit, one of `lsqfit`'s advanced features.)\n",
    "\n",
    "The information under \"Settings:\" mostly has to do with the internal workings of the fitter, and can be safely ignored unless you are running into numerical problems with doing the fit itself.\n",
    "\n",
    "Note that although calling `print(fit_result)` gives us a nice text-formatted report, `fit_result` itself is actually an object that contains other information in the form of properties (accessed with `.` notation.)  In the cell below, __try accessing the following properties__:\n",
    "\n",
    "* `fit_result.chi2`: Un-reduced value of $\\chi^2$ at best-fit point.\n",
    "* `fit_result.dof`: The number of degrees of freedom.\n",
    "* `fit_result.p`: Dictionary of best-fit parameters as gvars.\n",
    "* `fit_result.x` and `fit_result.y`: The data used in the fit.  (Often useful to use later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-741be1834d204e10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34ac980d11d68c18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part C\n",
    "\n",
    "Now, a little more advanced analysis.  When we do a fit, even if the chi-squared seems good (but definitely if it isn't), a good practice is to look at the __fit residuals__.  These are defined as\n",
    "\n",
    "$$\n",
    "r_i = y_i - f(x_i, \\mathbf{a}^\\star)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a}^\\star$ are the best-fit parameters.  (You can also try to normalize by the error bars $y_i$, which you can access using `gv.sdev()`.  This makes it possible to read off the \"chi-squared-by-eye\" contribution from each point - but the plot will look similar without the normalization.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c05876c78d387831",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "resids = (fit_result.y - PE_model(fit_result.x, fit_result.p)) / gv.sdev(fit_result.y)\n",
    "print(resids)\n",
    "plot_gvar(f_data, resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf802a996f55ebcb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hopefully, you will see that the points are scattered around zero with no obvious systematic pattern - a good sign that our model is indeed successfully describing the data.\n",
    "\n",
    "Next, __convert your fit results to an estimate for Planck's constant__.  By working in volts we've been implicitly dividing everything by the electron charge $e = 1.602 \\times 10^{-19}$ C, so you'll need to multiply the parameters through by that.  Don't forget to account for the factor of $10^{14}$ we pulled out of the frequency data!  The real value is\n",
    "\n",
    "$$\n",
    "h = 6.626 \\times 10^{-34}\\ \\rm{J} \\cdot \\rm{s}\n",
    "$$\n",
    "\n",
    "How close is your result?  Is the more precise value of $h$ within the error bar of your estimate?\n",
    "\n",
    "Using `fit_result.p`, calculate the ratio of your two model parameters, i.e. calculate $\\Phi / h$.  Not worrying about units or the $10^{14}$, if I go back to the fit itself and take the ratio with standard error propagation, I find `4.96(94)`.  But if you use `fit_result.p`, __the error on the ratio should be much smaller.__ Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ff4b1bd469239ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1447b8c4b810e368",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, implement the two alternative models, constant and quadratic:\n",
    "\n",
    "$$\n",
    "eV_{\\rm stop} = C \\\\\n",
    "eV_{\\rm stop} = kf^2 - \\Phi\n",
    "$$\n",
    "\n",
    "Run the nonlinear fits for both cases against the same data.  How do the p-values (\"Q\"-values) compare to the linear model?  Can you reject either or both alternative models based on this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ba11e4e1f82facc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-384953be1cb9fc4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part D (optional challenge) - fitting to exponential decay\n",
    "\n",
    "The exercise above was a nice introduction, but linear fits are really easy: they're safe from a lot of the problems that can appear in more general non-linear fits.  Let's step up the difficulty and try to fit an _exponential decay_.\n",
    "\n",
    "The cell below reads in a sequence of random decays - actually, they're generated using the Monte Carlo for radioactive decay from the homework.  The data consists of time (in seconds) and a set of particle counts from repeated trials.  (We'll see how to read data files next week - this is a little preview!)\n",
    "\n",
    "As a reminder, our model for the expected number of particles remaining is:\n",
    "\n",
    "$$\n",
    "N(t) = N_0 e^{-t/\\tau}.\n",
    "$$\n",
    "\n",
    "We have a partial sample of the data, running from 10 to 25 seconds, so we'll need to fit to determine both the size of the initial sample $N_0$ and the lifetime $\\tau$.\n",
    "\n",
    "First, __run the cell below__ to import the data and plot it.  (The data file isn't _too_ complicated, but it might be time-consuming to set up so I've provided the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7719410f00af33e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "remote_path = \"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/tutorials/tut23/decay_one_dataset.csv\"\n",
    "local_dir = \"/content/\"\n",
    "# local_dir = \"./\" # uncomment this line if you aren't running in Colab\n",
    "local_path = f\"{local_dir}decay_one_dataset.csv\"\n",
    "\n",
    "if not os.path.exists(local_path):\n",
    "    urllib.request.urlretrieve(remote_path, local_path)\n",
    "\n",
    "if local_dir not in sys.path:\n",
    "    sys.path.append(local_dir)\n",
    "    \n",
    "ds = np.genfromtxt('decay_one_dataset.csv')\n",
    "t_data = ds[:,0]\n",
    "decay_data = gv.dataset.avg_data(ds[:,1:].T)\n",
    "print(decay_data)\n",
    "plot_gvar(t_data, decay_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da00b1f63e828e3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now write a fit model function, then __run the nonlinear fit__ with initial guess $500$ for the initial abundance and $0.1$ for the lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-37256385f563f7ec",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fcn_exp_one(t, a):\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af834d71aefac3fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should find that your model is an extremely poor description of the data!  This doesn't mean your model function is wrong; it's actually a symptom of numerical instability, which exponential functions are notorious for.  __Try the fit again in the cell below__, changing the initial guess to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3cd25698ee2aea5f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43b4553312eb08cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This is from our model of nitrogen-16, so the correct lifetime is 10.286 seconds and the correct initial sample size was 1000 particles.  Hopefully your fit now agrees with those values and has a good p-value!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
