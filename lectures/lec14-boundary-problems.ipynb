{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c410a18bc886326b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Solving boundary-value ODEs\n",
    "\n",
    "<a href=\"http://www.physics.ucla.edu/icnsp/Html/spong/spong.htm\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/stellarator.jpg\" width=500px /></a>\n",
    "\n",
    "\n",
    "## PHYS 2600: Scientific Computing\n",
    "\n",
    "## Lecture 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of numerical ODEs\n",
    "\n",
    "In math class, you learn a ton of vocabulary for _classifying_ ODEs: homogeneous, linear, separable...\n",
    "\n",
    "For numerical solution, _we don't care_ about most of those math-class labels!  Instead, only one categorical division really matters for numerical ODEs:  __initial value problems__ (IVPs) versus __boundary value problems__ (BVPs). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/bvp-vs-ivp.png\" width=600px />\n",
    "\n",
    "In an IVP, we set all boundary conditions at a _single_ value of independent variable $x = x_0$ - the __initial value__.  In a BVP, boundary conditions are present at _two or more_ values of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now know how to solve IVPs by _iteration_: start at $x_0$, discretize, find the solution by stepping out from $x_0$.\n",
    "\n",
    "In general, __boundary-value problems are harder__ than IVPs, because they require \"non-local\" information about the solution.  If we try to step out from one boundary, we don't know if the solution will match at the other boundary.\n",
    "\n",
    "Fortunately, hard doesn't mean impossible!  There are two common approaches to solving boundary-value ODEs: __shooting__, and __relaxation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shooting\n",
    "\n",
    "Shooting exploits the fact that we _do_ know how to solve an ODE from an initial value, so we start by _partly_ matching the boundary conditions.\n",
    "\n",
    "Consider a second-order ODE with $y(0) = 0$ and $y(x_1) = y_1$.  Whatever the final solution is, it can _also_ be uniquely described by the initial conditions $y(0) = 0, y'(0) = c$ for some unknown $c$.  So we can just _guess_ some $c$, \"shoot\" the solution from $y(0)$ to $y(x_1)$ by solvingÂ numerically, and see how close we get!\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/bvp-shooting.png\" width=600px />\n",
    "\n",
    "Now the problem is reduced to adjusting $c$ until we hit the other boundary.  In fact, for any given $c$ we have $y(x_1) = F(c)$, so this is just a root-finding problem for the (black box) function $F(c)$ - and we know how to solve those!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relaxation\n",
    "\n",
    "Shooting has a couple of drawbacks; the most significant is _instability_.  If our solutions are highly sensitive to the initial guess, it may be very hard to make shooting converge.\n",
    "\n",
    "A more robust techinque is __relaxation__, which operates _globally_ on the whole solution between our boundaries at 0 and $x_1$.  We again start with an initial guess, but our guess is for a complete $y(x)$ (ideally, one that matches the two boundaries, or at least comes close!)  \n",
    "\n",
    "Our guess $y(x)$ will fail to satisfy the differential equation, but if we can propose a \"relaxation step\" that puts it _closer_ to solving the ODE, then we can iterate towards an actual solution.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/bvp-relaxation.png\" width=500px />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Implementing relaxation in practice is very tricky, and the algorithms are quite complex, so I'm not going to go into them.  (The ever-popular _Numerical Recipes_ has all of the details you could ever want.)\n",
    "\n",
    "There is one relaxation-based BVP solver I know of: once again in SciPy, the function `scipy.integrate.solve_bvp`.  The documentation says that this function using a \"collocation algorithm\", which is a fancy way to do relaxation where we express the solution as a sum over a number of simple functions with unknown coefficients.\n",
    "\n",
    "On the other hand, I don't know of any existing BVP solver that uses shooting in Python!  But that's alright, because shooting is just a simple combination of an IVP solver and a root-finder, and we have plenty of both to choose from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/clint-relax.jpg\" width=300px style=\"float:right;\" />\n",
    "\n",
    "It might be tempting to use the built-in SciPy solver first, but it's definitely harder to use and understand than applying shooting.  Most professional numerical analysts go by the gunslinger's motto: \"__shoot first, relax later.__\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Root finding\n",
    "\n",
    "The basic problem is as follows: given the equation\n",
    "\n",
    "$$\n",
    "f(x) = 0,\n",
    "$$\n",
    "\n",
    "find all values $\\{x_r\\}$ that satisfy the equation.  (The solutions $\\{x_r\\}$ are the _roots_ or _zeroes_ of $f(x)$.)\n",
    "\n",
    "Many problems in physics and math can be framed as root-finding!  Solving any equation $f(x) = g(x)$ is just root-finding $f(x) - g(x) = 0$.  Calculating $\\sqrt{a}$ is equivalent to finding the roots of the equation\n",
    "\n",
    "$$\n",
    "y^2 - a = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "There are many algorithms for root finding; here are three examples.\n",
    "\n",
    "* __Brute force__ checks every number over a certain interval to see which ones satisfy the equation. \n",
    "* __Bisection__ uses a simple math argument to quickly narrow down a single root.\n",
    "* __Newton-Raphson__ uses the derivative $f'(x)$ to extrapolate to where the nearest root is.  (\"Heron's method\" for the square root is actually Newton-Raphson!)\n",
    "\n",
    "All of these methods are __iterative methods__: start with an initial guess $x_0$, and then improve the guess repeatedly ($x_0 \\rightarrow x_1 \\rightarrow x_2...$) until it's \"close enough\" to correct.  The __stopping condition__ defines when we are \"close enough\" in math terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "There are Python modules that implement all of these and many more well-known algorithms for root finding.  Many are available in the [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html#root-finding) sub-module.\n",
    "\n",
    "One rule of good programming is: _never re-invent the wheel!_  But this is a class, so our job is to see how these algorithms work on the inside, to save you from trouble when you try to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The stopping condition is usually given in terms of some __error tolerance__ $\\epsilon$.  This is, ideally, the error on our numerical solution: in other words, our answer $x_i$ and the true root $x_r$ should satisfy\n",
    "\n",
    "$$\n",
    "|x_i - x_r| < \\epsilon.\n",
    "$$\n",
    "\n",
    "Unfortunately, we usually can't test this directly - we don't know $x_r$!  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Instead, we can check how close $f(x_i)$ is to zero using our bound:\n",
    "\n",
    "$$\n",
    "|f(x_i) - f(x_r)| = |f(x_i)| < \\epsilon.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Taylor expanding about $x_r$ lets us relate this to what we want:\n",
    "\n",
    "$$\n",
    "|f(x_i)| \\approx |f'(x_r)| |x_i - x_r| < \\epsilon\n",
    "$$\n",
    "\n",
    "which is _almost_ the same as $|x_i - x_r| < \\epsilon$; we just have an extra constant, $|f'(x_r)|$.  As long as it isn't too big, bounding $|f(x_i)|$ is a good bound on the error for estimating $x_r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/root-find-sketch.png\" width=500px style=\"float:right;margin:20px\" />\n",
    "\n",
    "Here's a more visual way to see what is going on: we want the _horizontal_ distance to the root, but the best we can usually do is the _vertical_ distance.  They are related by the slope of $f(x)$ at $x_r$.\n",
    "\n",
    "For most practical uses, we simply __set the error tolerance to be much smaller than any other sources of error__ in our problem, so we don't care if we're off by $\\epsilon$ or $2\\epsilon$.  (If making $\\epsilon$ really small becomes too expensive, then you need to worry about this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Tutorial 14\n",
    "\n",
    "Let's solve the Schrodinger equation!  Connect to the server and load up `tut14`.\n",
    "\n",
    "## Additional reading\n",
    "\n",
    "If you're hungry for more advanced root-finding algorithms and an in-depth study of the problem in general, chapter 9 of the famous _Numerical Recipes_ has all the root-finding you can handle!  (The code examples, however, are not in Python.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
