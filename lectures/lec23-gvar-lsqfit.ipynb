{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Error propagation and least-squares fitting\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/planck_TT.png\" width=500px />\n",
    "\n",
    "## PHYS 2600: Scientific Computing\n",
    "\n",
    "## Lecture 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %pip install -q gvar lsqfit # uncomment this line if you're running in Colab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gvar as gv\n",
    "import lsqfit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling error bars with `gvar`\n",
    "\n",
    "Today, we'll cover two Python modules that are essential for working with real data - `gvar` for \"Gaussian random variables\", i.e. numbers with error bars, and `lsqfit` for least-squares fits.  These tools can save you some time and headaches in the physics lab (either a class, or a real research lab!)\n",
    "\n",
    "Suppose you measure the length `L` and height `h` of an inclined plane, to find the angle.  You measure in cm, with an uncertainty of 1 mm.  To input your measurements into `gvar` (short name `gv`), you write them like you would in a lab notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = gv.gvar('4.0(1)') # = 4.0 +- 0.1\n",
    "h = gv.gvar('2.0(1)') # = 2.0 +- 0.1\n",
    "print(L,h)\n",
    "\n",
    "L = gv.gvar(4.0, 0.1) # = 4.0 +- 0.1\n",
    "h = gv.gvar(2.0, 0.1) # = 2.0 +- 0.1\n",
    "print(L,h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`number(error)` is short-hand for \"`number` has an uncertainty of `error`\".  This means we think of `number` as being represented by a normal distribution - I won't review this formalism.  but this is what \"Gaussian variable\" means.  The central limit theorem justifies a lot of this treatment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the uses of Gaussian variables is __error propagation__ when we take functions them.  I'll remind you of the most general formula for two variables: if $z = f(x,y)$, then\n",
    "\n",
    "$$\n",
    "\\sigma_z = \\sqrt{\\left( \\frac{\\partial f}{\\partial x}\\right)^2 \\sigma_x^2 + \\left( \\frac{\\partial f}{\\partial y} \\right)^2 \\sigma_y^2 + 2 \\left( \\frac{\\partial f}{\\partial x} \\right) \\left( \\frac{\\partial f}{\\partial y} \\right) \\sigma_x \\sigma_y \\rho_{xy}}\n",
    "$$\n",
    "\n",
    "where $\\sigma_x$ and $\\sigma_y$ are the error bars on $x$ and $y$ respectively, and $\\rho_{xy}$ is the __correlation coefficient__ between $x$ and $y$ (zero if they are independent.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the most powerful features of `gvar` is that _it handles error propagation automatically!_  For example,\n",
    "we can get the angle of our ramp as a function of `L` and `h`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.arctan2(L,h)\n",
    "print(theta, theta * 180 / np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our original inputs `h` and `L` are uncorrelated by default.  But since `theta` was produced _from_ `L` and `h`, it does have non-zero correlation with both quantities.  We can see the correlation with `gv.corr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gv.corr(L,h))\n",
    "print(gv.corr(theta,h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A powerful feature of `gvar` is that it __automatically remembers and propagates all correlations__!  This is important to get things right, and the difference (vs. ignoring correlations) is often large.\n",
    "\n",
    "For example, suppose we want to reconstruct the original $L = h \\tan \\theta = 4.0(0.1)$.  Look at what happens with a new `gvar` (uncorrelated) versus using `h` and `theta` with correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h*np.tan(gv.gvar('1.107(22)')))\n",
    "print(h*np.tan(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Including the correlations gives us back the original, correct error value for `L`!  (Of course, correlations only change the _error_ propagation, so the mean value is the same either way.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometimes measurements come with a known error, but often the error is inferred from statistics: we take a number of repeated measurements of some random process, and then compute the standard deviation $\\sigma$ to get an error bar.  This can be done automatically by `gv.dataset.avg_data()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = np.random.normal(1.3,0.4,size=100)\n",
    "print(some_data[:10])\n",
    "gv.dataset.avg_data(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that the width of the `gvar` is _smaller_ than the width 0.4 I started with - exactly $\\sqrt{N}$ smaller. (`avg_data` computes the standard error.)\n",
    "\n",
    "A powerful feature of `gv.dataset.avg_data()` is that it can also compute __correlations__ from raw data, if you have synchronized measurements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least-squares fitting\n",
    "\n",
    "Least-squares fitting is a very common tool in physics for: (1) determining what model describes some data, and (2) inference of model parameters (physical constants.)  I'll assume you've seen this before, so I'll just remind you of some key concepts.\n",
    "\n",
    "The starting point of least-squares fitting is defining the _chi-squared function_,\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_i \\left(\\frac{y_i - f(x_i, \\mathbf{a})}{\\sigma_i} \\right)^2\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a}$ is a vector of model parameters.  The __minimum possible__ $\\chi^2_{\\star}$ is the \"best-fit\" $\\chi^2$ statistic, with model parameters ${\\mathbf{a}}_{\\star}$ (the \"best fit parameters.\")  Standard numerical minimization methods work to find the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/anscombes_quartet.png\" width=400px style=\"float:right;\" />\n",
    "\n",
    "Although $\\chi^2$ is useful, because it's a _global summary statistic_, looking at $\\chi^2$ alone can obscure local features in your data or model that you might not expect!\n",
    "\n",
    "The most famous pathological example of this effect is [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet), shown to the right (plot from the `seaborn` [Python module docs](https://seaborn.pydata.org/examples/anscombes_quartet.html).)  Assuming the error bars on all points are the same, a linear fit to all four data sets will yield _exactly the same fit parameters and $\\chi^2$ statistic!_  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(If that's not convincing yet, we get exactly the same linear fit from [the Datasaurus shown below](https://dabblingwithdata.wordpress.com/2017/05/03/the-datasaurus-a-monstrous-anscombe-for-the-21st-century/).)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/datasaurus-main.png\" width=300px style=\"float:left;margin:10px;\"/>\n",
    "\n",
    "The moral: quick and easy statistical tests are no replacement for looking at your data!  __Plot your data and model against each other!__  Plotting _residuals_ (data minus model) can be a good way to make deviations stand out by eye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To evaluate goodness of fit, we can compute the __p-value__ from the expected chi-squared distribution.  Most fitting packages will do this for you.  A good __rule of thumb__ is to compute $\\chi^2 / N_{\\rm dof}$, the \"reduced chi-square\".  A good model will yield $\\chi^2 / N_{\\rm dof} \\lesssim 1$, no matter what $N_{\\rm dof}$ is.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are multiple options available in Python for doing fits to data - minimizing $\\chi^2$, reporting best-fit parameters, etc.  For example, `scipy` contains `scipy.optimize.curve_fit` for non-linear least squares, `numpy` has `numpy.polyfit`, and the scikit-learn module (focused on Machine Learning) also has a number of routines for model regression.\n",
    "\n",
    "However, most of these tools are not specialized to the most common form of model fitting in physics: fitting _arbitrary_ (not necessarily linear!) functions to data _with errors_.  (In modern versions of SciPy, `curve_fit` can handle this problem.)\n",
    "\n",
    "For this class, I've chosen to teach `lsqfit` - it is from the same author as `gvar`, and so it automatically uses `gvars` to deal with errors and error propagation.  It has the best \"ergonomics\" for physics problems, in my experience.  But as a warning: it is much less well known than the equivalent `scipy` tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tutorial 23\n",
    "\n",
    "Let's start the tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
