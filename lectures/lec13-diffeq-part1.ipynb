{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c410a18bc886326b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Derivatives and finite differences\n",
    "\n",
    "<!-- <a href=\"http://depts.washington.edu/astron/research/n-body-shop/\" target=\"_blank\"><img src=\"img/cosmo25.HR.gold.jpg\" width=400px/></a> -->\n",
    "<a href=\"http://depts.washington.edu/astron/research/n-body-shop/\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/cosmo25.HR.gold.jpg\" width=400px/></a>\n",
    "\n",
    "## PHYS 2600\n",
    "\n",
    "## Lecture 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discrete derivatives\n",
    "\n",
    "From your calculus textbook, a derivative is defined as follows:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = \\lim_{\\epsilon \\rightarrow 0} \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}.\n",
    "$$\n",
    "\n",
    "In fact, the derivative is often first introduced in terms of a __secant line__, which describes the slope of $f(x)$ between any two points $x_1$ and $x_2$:\n",
    "\n",
    "$$\n",
    "b = \\frac{f(x_2) - f(x_1)}{x_2 - x_1},\n",
    "$$\n",
    "\n",
    "and we get the derivative as the slope of the __tangent line__ as $x_2$ is brought together with $x_1$.  \n",
    "\n",
    "<!-- <img src=\"img/sec-to-tan.gif\" width=400px /> -->\n",
    "<img src=\"https://raw.githubusercontent.com/wlough/CU-Phys2600-Fall2025/main/lectures/img/sec-to-tan.gif\" width=400px />\n",
    "\n",
    "(Nice animation borrowed from [this UCSB page on calculus](http://clas.sa.ucsb.edu/staff/lee/Secant,%20Tangent,%20and%20Derivatives.htm).)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This gives an obvious way to define a discrete approximation of $df/dx$: just use the secant definition!\n",
    "\n",
    "$$\n",
    "f'(x_i) \\approx y'_i = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} = \\frac{y_{i+1} - y_i}{a}\n",
    "$$\n",
    "\n",
    "where the second `=` is true when our points have constant spacing $a$.  $y'_i$ is a _discrete approximation_ to $f'(x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that discretization we use is a *choice*.  We could have instead done this:\n",
    "\n",
    "$$\n",
    "f'(x_i) \\approx y'_i = \\frac{y_i - y_{i-1}}{a}.\n",
    "$$\n",
    "\n",
    "This is called the __backward difference__ definition; the one we started with was the __forward difference__.  They are identical (and equal to $f'(x_i)$) when $a \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In fact, these aren't the only options - there are many ways to build a secant line!  We can also use a __central difference__:\n",
    "\n",
    "$$\n",
    "f'(x_i) \\approx y'_i = \\frac{y_{i+1} - y_{i-1}}{2a}\n",
    "$$\n",
    "\n",
    "(note the 2 in the denominator.)  This also becomes the usual derivative as the spacing $a$ between the points vanishes.\n",
    "\n",
    "In fact, this version is often better: the _discretization error_ $f'(x_i) - y'_i$ is proportional to $a^2$, versus $a$ for forward/backward difference.  (The jargon you will see is that this version is __second-order accurate__.) \n",
    "\n",
    "There are approximations with even smaller discretization error, but they require using more than two points at once.  I won't go into them here - again, the best way to reduce error is just to make $a$ smaller, unless your problem is very expensive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `np.gradient()` function calculates the second-order accurate version of $y'_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(0,2*np.pi,200)\n",
    "dx = x[1]-x[0]\n",
    "y = np.sin(x)\n",
    "dy_dx = np.gradient(y, x[1]-x[0]) # dy/dx = cos(x)\n",
    "plt.plot(x, y, color='k')\n",
    "plt.plot(x, dy_dx, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we move on, a general comment: _you are better off calculating $f'(x)$ analytically_ whenever you can!  Numerical derivatives e.g. `np.gradient()` are for situations where you _can't_ do this - $f(x)$ is a \"black box\", or is just very complicated.  (Solving differential equations is an example where you don't know $f(x)$ in advance!)\n",
    "\n",
    "There is a growing interest in __automatic differentiation__, which calculates _analytic_ derivatives from your Python functions: I won't talk more about this, but have a look at [JAX](https://jax.readthedocs.io/en/latest/) if you're interested.  (JAX is a foundational library for machine learning.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discrete differential equations\n",
    "\n",
    "Consider a first-order ODE of the form:\n",
    "\n",
    "$$\n",
    "\\frac{du}{dt} = F(u(t), t)\n",
    "$$\n",
    "\n",
    "With the initial condition $u(0) = c$. This type of differential equation is known as an **initial value problem** (IVP).\n",
    "\n",
    "If the right-hand side is just $F(t)$, then the solution is simple:\n",
    "\n",
    "$$\n",
    "u(t) = c +  \\int_0^t d\\tau\\ F(\\tau)\n",
    "$$\n",
    "\n",
    "i.e. the problem just reduces to a numerical integral.  The case where $u(t)$ appears on the right-hand side is _much_ more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To solve more generally, we discretize!  Replacing $t \\rightarrow \\{t_i\\}$, the ODE becomes\n",
    "$$\n",
    "u'(t_i) = F(u_i, t_i)\n",
    "$$\n",
    "where we define $u_i = u(t_i)$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We don't know $u'(t)$, but we can approximate it with one of our discrete derivatives! Replacing $u'(t)$ with the forward difference gives us:\n",
    "$$\n",
    "\\frac{u_{i+1} - u_i}{\\Delta t} = F(u_i, t_i)\n",
    "$$\n",
    "This is now a __difference equation__ (discrete verison of a differential equation.) Note that we had to **choose** which finite difference approximation to use. Different approximations of $u'(t)$ give different difference equations. Replacing the derivative with a forward difference is called the \"__forward Euler method__\" or \"__explicit Euler method__\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "At first glance, the difference equation might not seem any better than the original differential equation, but rearranging terms gives us a formula\n",
    "$$\n",
    "u_{i+1} = u_i + F(u_i, t_i) \\Delta t \n",
    "$$\n",
    "for $u_{i+1}$ (the function evaluated at $t_{i+1}=t_{i} + \\Delta t$) in terms of $u_i$ (the function evaluated at $t_{i}$). In other words, it gives us a way to compute future values of the function future from past values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "How can we use this to (approximately) solve the IVP? Remember our initial condition: \n",
    "$$\n",
    "u_0=c\n",
    "$$\n",
    "Plugging this in lets us determine $u_1$, then plugging in $u_1$ gives $u_2$...\n",
    "$$\n",
    "u_1 = u_0 + F(u_0, t_0) \\Delta t\n",
    "...\n",
    "$$\n",
    "$$\n",
    "u_2 = u_1 + F(u_1, t_1) \\Delta t\n",
    "$$\n",
    "\n",
    "and so on!  Each step in this process is called a **timestep**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "To reiterate, here is the full forward Euler method algorithm for solving $u'(t) = F(u(t), t)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "__(1)__ Discretize time $t \\rightarrow \\{t_i\\}$ and replace $u'(t)$ with the forward-difference $(u_{i+1} - u_i) / \\Delta t$.  This gives:\n",
    "\n",
    "$$\n",
    "u_{i+1} = u_i + F(u_i, t_i)\\ \\Delta t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "__(2)__ Iterate over the range of all $\\{t_i\\}$, starting at $t_0$.  Accumulate an array or list of results $\\{u_i\\}$:\n",
    "  - For each $t_i$, use the discrete equation with $u_i$ to find $u_{i+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "__(3)__ Return the result for $\\{u_i\\}$, which is a discrete approximation to the solution $u(t)$ over the discrete interval $\\{t_i\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The sort of timestepping process is common to most numerical IVP solvers! Variations mostly just involve changing the finite difference formulas used to approximate derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In general, the quality of our numerical solution will depend on both the **stepsize** $\\Delta t$ and on the specific finite difference formulas used to approximate derivatives. There are plenty of numerical schemes which outperform forward Euler, but Euler often does fine if you use a small enough timestep. It is always a good idea to **start with the simple approach** and only resort to more sophisticated methods when the simple method fails.\n",
    "\n",
    "If Forward Euler doesn't seem to be working for a problem, you may want to try one of the higher order [__Runge-Kutta__ methods](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods). They're a bit trickier to implement, but they also tend to give much better approximations. We're not going to dig into the gory details of implementing more sophisticated ODE solvers our selves (yet!). The [scipy module](https://docs.scipy.org/doc/scipy/reference/integrate.html) already has a lot of the RK methods pre-built and ready to use!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "First-order might seem kind of restrictive: what about dealing with higher-order equations, like we might see in kinematics?\n",
    "$$\n",
    "F(t) = m \\frac{d^2x}{dt^2} \\Rightarrow \\frac{d^2x}{dt^2} =  \\frac{F(t)}{m}\n",
    "$$\n",
    "Using a finite difference approximation of the second derivative is one option, but there are many others!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Another approach is exactly what we do on paper: we can work directly with the velocity $v = dx/dt$! This turns our **second order ODE in one variable** into a **first order ODE in two variables**:\n",
    "\n",
    "$$\n",
    "\\frac{dv}{dt} = \\frac{F(t)}{m}\n",
    "$$\n",
    "$$\n",
    "\\frac{dx}{dt} = v(t)\n",
    "$$\n",
    "\n",
    "This trick works for __any__ system of ODEs! You can always get rid of higher order derivatives in an ODE by introducing new variables.\n",
    "\n",
    "\n",
    "Replacing $\\frac{dv}{dt}$ and $\\frac{dx}{dt}$ with forward difference approximations gives a pair of difference equation:\n",
    "$$\n",
    " v_{i+1} = v_i + \\frac{F_i}{m} \\Delta t\n",
    "$$\n",
    "$$\n",
    " x_{i+1} = x_i + v_i \\Delta t\n",
    "$$\n",
    "This sort of system can be solved using the same timestepping method from the previous example: start at $i=0$, with two initial conditions $(x_0, v_0)$, solve both equations to find $x_1$ and $v_1$, then repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tutorial 13\n",
    "\n",
    "Open up `tut13`, and we'll work through a numerical solution step by step."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
